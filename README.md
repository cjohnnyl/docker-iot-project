# ğŸš€ Projeto IoT â€“ Viveiro de Eucalipto  
### *Streaming de dados em tempo real com Kafka, Spark e Postgres*

Este projeto simula um cenÃ¡rio de **monitoramento IoT em um viveiro de eucaliptos**, onde sensores enviam leituras contÃ­nuas de **temperatura do solo e umidade**, publicadas em tempo real em um **tÃ³pico Kafka**, processadas pelo **Apache Spark Structured Streaming** e armazenadas no **PostgreSQL**.

---

## ğŸ§  Arquitetura

```
[Sensores simulados]
        â†“
   Kafka (Producer)
        â†“
  Spark Structured Streaming (Consumer)
        â†“
   PostgreSQL (armazenamento final)
```

**Fluxo resumido:**
1. O *producer* gera dados de sensores simulados (IoT).  
2. As mensagens sÃ£o publicadas continuamente no tÃ³pico `iot_topic` do Kafka.  
3. O Spark consome o tÃ³pico em *streaming*, calcula mÃ©dias de temperatura/umidade e grava no Postgres.  
4. O Postgres mantÃ©m as leituras processadas, prontas pra anÃ¡lise ou visualizaÃ§Ã£o.

---

## ğŸ§° Tecnologias

| Componente | FunÃ§Ã£o | VersÃ£o |
|-------------|--------|--------|
| ğŸ³ Docker Compose | OrquestraÃ§Ã£o dos serviÃ§os | latest |
| ğŸ¦ Zookeeper | CoordenaÃ§Ã£o do cluster Kafka | 7.2.1 |
| ğŸ“¡ Kafka | Mensageria em tempo real | 7.2.1 |
| ğŸ Python | SimulaÃ§Ã£o de sensores (producer) | 3.10 |
| âš¡ Apache Spark | Processamento em streaming | 3.5.0 |
| ğŸ˜ PostgreSQL | Armazenamento relacional | 14 |

---

## âš™ï¸ Como rodar o projeto

1ï¸âƒ£ **Clone o repositÃ³rio**
```bash
git clone https://github.com/cjohnnyl/docker-iot-project.git
cd docker-iot-project
```

2ï¸âƒ£ **Suba os containers**
```bash
docker compose up -d --build
```

Isso vai iniciar:
- `zookeeper`
- `kafka`
- `postgres`
- `producer`
- `spark`

> **ObservaÃ§Ã£o:** o container do Spark sobe â€œociosoâ€. Ã‰ preciso iniciar manualmente o *consumer* (prÃ³ximo passo).

3ï¸âƒ£ **Inicie o consumer do Spark (manualmente)**  
Abra um terminal no container do Spark e execute o `spark-submit`:

```bash
# entrar no container
docker exec -it spark bash

# dentro do container, iniciar o consumer
/opt/spark/bin/spark-submit   --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.postgresql:postgresql:42.6.0   /opt/spark-app/consumer/consumer.py
```

> Dica: se o Kafka ainda estiver subindo, vocÃª pode ver erros temporÃ¡rios como `NoBrokersAvailable`. Espere alguns segundos e rode o comando novamente.

4ï¸âƒ£ **(Opcional) UI do Spark**  
Com o consumer rodando, acesse a UI em: `http://localhost:4040` (porta jÃ¡ mapeada no `docker-compose`).

---

## ğŸ”„ Verificando o funcionamento

### 1ï¸âƒ£ Logs do Producer (envio de dados IoT)
Mostra as mensagens sendo publicadas no tÃ³pico Kafka.

```bash
docker logs -f producer
```

SaÃ­da esperada:
```
[Producer] Sent: {'sensor_id': 's1', 'estufa_id': 'EUCALIPTO_01', 'soil_temp_c': 28.7, 'humidity': 67.4, 'timestamp': '2025-11-10T00:22:35.970269'}
```

ğŸ“˜ *Essas mensagens simulam sensores reais transmitindo dados de temperatura e umidade.*

---

### 2ï¸âƒ£ Logs do Spark (consumo e gravaÃ§Ã£o no Postgres)
Mostra o Spark Structured Streaming consumindo o tÃ³pico Kafka e gravando micro-batches no banco.

```bash
docker logs -f spark
```

SaÃ­da esperada:
```
âœ… Batch 0 gravado no Postgres com sucesso.
âœ… Batch 1 gravado no Postgres com sucesso.
âœ… Batch 2 gravado no Postgres com sucesso.
```

ğŸ“˜ *Aqui o Spark confirma que cada batch de dados foi processado e persistido no PostgreSQL.*

---

### 3ï¸âƒ£ Consultando os dados no PostgreSQL

Listar as tabelas:
```bash
docker exec -it postgres psql -U postgres -d iot_data -c "\dt"
```

Consultar as Ãºltimas leituras:
```bash
docker exec -it postgres psql -U postgres -d iot_data -c "SELECT * FROM iot_readings ORDER BY processed_at DESC LIMIT 10;"
```

Exemplo de saÃ­da:
```
 estufa_id   |  avg_soil_temp_c  |  avg_humidity  |      processed_at       
--------------+-------------------+----------------+-------------------------
 EUCALIPTO_01 | 26.04             | 65.40          | 2025-11-10 00:22:40
 EUCALIPTO_01 | 26.03             | 65.32          | 2025-11-10 00:22:30
 ...
```

ğŸ“˜ *Esses dados sÃ£o agregados em tempo real pelo Spark a partir das mensagens Kafka.*

---

## ğŸ§© Estrutura do Projeto

```
docker-iot-project/
â”‚
â”œâ”€â”€ docker-compose.yml        # Orquestra todos os containers
â”œâ”€â”€ producer/
â”‚   â”œâ”€â”€ producer.py           # Gera e envia dados IoT para o Kafka
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ consumer.py           # Consome o tÃ³pico Kafka e grava no Postgres
â”‚   â””â”€â”€ Dockerfile            # Imagem customizada do Spark
â”‚
â””â”€â”€ README.md                 # Este arquivo
```

---

## ğŸ“Š DemonstraÃ§Ã£o sugerida (para o avaliador)

Durante a apresentaÃ§Ã£o, seguir esta ordem:

1. **Mostrar o producer publicando dados IoT**
   ```bash
   docker logs -f producer
   ```
   â†’ Sensores simulados enviando dados.

2. **Mostrar o spark consumindo e processando**
   ```bash
   docker logs -f spark
   ```
   â†’ Batches processados e gravados no banco.

3. **Mostrar o Postgres com dados atualizados**
   ```bash
   docker exec -it postgres psql -U postgres -d iot_data -c "SELECT * FROM iot_readings ORDER BY processed_at DESC LIMIT 10;"
   ```
   â†’ Leituras recentes, atualizadas a cada batch (~10 segundos).

4. (Opcional) **Mostrar o fluxo pelo Kafka diretamente**
   ```bash
   docker exec -it kafka bash
   kafka-console-consumer --bootstrap-server kafka:9092 --topic iot_topic --from-beginning
   ```

---

## ğŸ§¾ ObservaÃ§Ãµes tÃ©cnicas

- O Spark Structured Streaming processa micro-batches a cada 10 segundos (`trigger interval = 10000ms`).
- O Postgres armazena a mÃ©dia de temperatura e umidade por estufa.
- Todos os serviÃ§os sÃ£o isolados via Docker, facilitando reprodutibilidade.
- Caso precise reiniciar:
  ```bash
  docker compose down
  docker compose up -d
  ```

---

## ğŸ’¡ PrÃ³ximos passos (opcional)
- Adicionar checkpoint no Spark (`.option("checkpointLocation", "/tmp/checkpoint")`).
- Automatizar a inicializaÃ§Ã£o do consumer ajustando o `CMD` no `Dockerfile` do Spark.
- Criar dashboard em Streamlit ou Grafana conectado ao Postgres.
- Persistir dados do Kafka e Postgres em volumes Docker.

---

## ğŸ‘¤ Autor
**Carlos Johnny Leite**  
Engenheiro de Dados  
ğŸ”— [github.com/cjohnnyl](https://github.com/cjohnnyl)
